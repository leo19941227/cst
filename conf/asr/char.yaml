expdir: ???
num_workers: 8

trainer:
  max_epochs: 100
  accumulate_grad_batches: 1
  gradient_clip_val: 1.0
  precision: 16
  log_every_n_steps: 100
save_steps: 1000
valid_metric: valid/wer
valid_higher_better: false

target: cst.models.asr.CtcASR

model:
  upstream_name: hubert
  tokenizer_name: char

  specaug_conf:
    adaptive: false
    adaptive_number_ratio: 0.04
    adaptive_size_ratio: 0.04
    max_n_time_masks: 20
    apply_time_warp: true
    apply_time_mask: true
    apply_freq_mask: true
    time_warp_window: 5
    time_mask_width_range: [0, 40]
    freq_mask_width_range: [0, 50]
    num_freq_mask: 4
    num_time_mask: 2

  downstream_conf:
    total_rate: -1
    module: 'LSTM'                        # 'LSTM'/'GRU'
    bidirection: True
    dim: [1024, 1024]
    dropout: [0.2, 0.2]
    layer_norm: [False, False]
    proj: [False, False]              # Linear projection + Tanh after each rnn layer
    sample_rate: [1, 1]
    sample_style: 'concat'                  # 'drop'/'concat'
  lr: 1.0e-4

data:
  train_conf:
    data_list: data/asr/librispeech/train_clean_100
    total_samples: 10000000
    shuffle: true 
    num_workers: ${num_workers}
  valid_conf:
    data_list: data/asr/librispeech/test_clean
    total_samples: ${data.train_conf.total_samples}
    shuffle: false
    num_workers: ${num_workers}

expdir: ???
num_workers: 8

trainer:
  max_epochs: 100
  accumulate_grad_batches: 1
  gradient_clip_val: 1.0
  precision: 16
  log_every_n_steps: 100
save_steps: 1000
valid_metric: valid/loss
valid_higher_better: false

target: cst.models.compress_ssl.CompressSSL

model:
  upstream_name: hubert
  autoencoder_conf:
    representation_size: 768
    latent_size: 8
    # -1: pos emb, -2: downsample, -3: upsample
    encoding_layer_sizes: [-1, 768, -2, 768, -2, 768, -2, 768, 768, 768]
    decoding_layer_sizes: [768, 768, 768, -3, 768, -3, 768, -3, 768]
    sample_posterior: true
  lr: 1.0e-5
  logvar_init: 0.0
  kl_weight: 1.0

data:
  train_conf:
    data_list: data/librispeech/train_960
    training: true
    max_samples: 320000
    min_samples: 48000
    total_samples: 20000000
    shuffle: true 
    num_workers: ${num_workers}
  valid_conf:
    data_list: data/librispeech/test_clean
    training: false
    total_samples: ${data.train_conf.total_samples}
    shuffle: false
    num_workers: ${num_workers}
